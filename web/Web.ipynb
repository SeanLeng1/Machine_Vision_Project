{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import cv2\n",
    "import os\n",
    "import glob \n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "from PIL import Image\n",
    "from copy import copy\n",
    "import splitfolders\n",
    "from torchsummary import summary\n",
    "# import scheduler for learning rate change\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pickle\n",
    "from flask import Flask, render_template, Response, request\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894152f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook can't import file. Copy the class structure of ensemble model from project.ipynb\n",
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, model4, num_classes=7):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.model4 = model4\n",
    "        self.model5 = model5\n",
    "        # Remove last linear layer\n",
    "        self.model1.fc = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "        self.model3.fc = nn.Identity()\n",
    "        self.model4.fc = nn.Identity()\n",
    "        self.model5.fc = nn.Identity()\n",
    "        # Create new classifier\n",
    "        #self.classifier = nn.Linear(4110, num_classes)\n",
    "        self.classifier = nn.Sequential(#nn.Dropout(0.5),\n",
    "                               #nn.Flatten(),\n",
    "                            nn.Linear(4110, 4096),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 1024),\n",
    "                            #nn.BatchNorm1d(1024),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.Dropout(0.5),\n",
    "                            nn.Linear(1024, 7))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x.clone())  # clone to make sure x is not changed by inplace methods\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = self.model2(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x3 = self.model3(x)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        x4 = self.model4(x)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        #x5 = self.model5(x)\n",
    "        #x5 = x5.view(x5.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = self.classifier(F.relu(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc95a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/\n",
    "import os\n",
    "os.chdir(\"C:/Users/jixua/OneDrive/Desktop/Machine learning package/Web\")\n",
    "emotion_model = torch.load('ensemble_model4_tmax150')\n",
    "app = Flask(__name__, template_folder = './templates')\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "global switch \n",
    "switch = 0\n",
    "global img\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize(48),\n",
    "    transforms.TenCrop(40),\n",
    "    transforms.Lambda(lambda crops : torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "emotion_model = emotion_model.to(device)\n",
    "#data_dir = 'C:/Users/jixua/OneDrive/Desktop/Machine learning package'\n",
    "#test_set = torchvision.datasets.ImageFolder(data_dir + '/archive/test', transform = valid_image_transform)\n",
    "labels_class = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a702eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real-time-detection using Google mediapipe\n",
    "def real_time_detection(model): \n",
    "    global img\n",
    "    if switch == 1:\n",
    "        #cap = cv2.VideoCapture(0)\n",
    "        #cap = camera\n",
    "        pTime = 0\n",
    "\n",
    "        mpFaceDetection = mp.solutions.face_detection\n",
    "        mpDraw = mp.solutions.drawing_utils\n",
    "        faceDetection = mpFaceDetection.FaceDetection(0.6)\n",
    "\n",
    "        while True:\n",
    "            if switch == 0:\n",
    "                break\n",
    "            success, img = camera.read()\n",
    "\n",
    "            imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = faceDetection.process(imgRGB)\n",
    "            #print(results)\n",
    "\n",
    "            if results.detections:\n",
    "                for id, detection in enumerate(results.detections):\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    ih, iw, ic = img.shape\n",
    "                    bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                    #print(bbox)\n",
    "\n",
    "                    cv2.rectangle(img, bbox, (255, 0, 255), 2)\n",
    "                    #cv2.putText(img, f'{int(detection.score[0] * 100)}%',\n",
    "                                #(bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN,\n",
    "                                #2, (255, 0, 255), 2)\n",
    "                    # process the crop_img\n",
    "                    #crop_img = img[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
    "                    #crop_img = img[int(bboxC.xmin * iw-10): int(bboxC.width * iw), int(bboxC.ymin * ih-10): int(bboxC.height * ih)]\n",
    "\n",
    "                    #cv2.imshow(\"crop\", crop_img)\n",
    "                    #crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "                    crop_img = Image.fromarray(img)\n",
    "                    crop_img = crop_img.crop(bbox)\n",
    "                    crop_img = crop_img.resize((48, 48))\n",
    "                    crop_img_tensor = test_image_transform(crop_img)\n",
    "                    crop_img_variable = Variable(crop_img_tensor.unsqueeze(0))\n",
    "                    crop_img_variable = crop_img_variable.to(device)\n",
    "                    bs, ncrops, c, h, w = crop_img_variable.shape\n",
    "    \n",
    "                    crop_img_variable = crop_img_variable.view(-1, c, h, w)\n",
    "                    output = model(crop_img_variable)\n",
    "                    output = output.view(bs, ncrops, -1)\n",
    "                    output = torch.sum(output, dim=1) / ncrops\n",
    "    \n",
    "                    h_x = torch.nn.functional.softmax(output, dim = 1).data.squeeze()\n",
    "                    probs, idx = h_x.sort(0, True)\n",
    "                    cv2.putText(img, '{}. probability:{:.3f}'.format(labels_class[idx[0].item()], probs[0]), (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN,\n",
    "                                1, (255, 0, 255), 2)\n",
    "\n",
    "            cTime = time.time()\n",
    "            fps = 1 / (cTime - pTime)\n",
    "            pTime = cTime\n",
    "            cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 2)\n",
    "            #cv2.imshow(\"Image\", img)\n",
    "            \n",
    "            (flag, encodedImage) = cv2.imencode(\".jpg\", img)\n",
    "            if not flag:\n",
    "                continue\n",
    "                \n",
    "            yield(b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + \n",
    "                bytearray(encodedImage) + b'\\r\\n')\n",
    "            \n",
    "            # q to quit the program\n",
    "            #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                #break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28303d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real-time detection using YOLO\n",
    "os.chdir(\"C:/Users/jixua/OneDrive/Desktop/Machine learning package/YOLO\")\n",
    "import torch\n",
    "import matplotlib as plt\n",
    "from model import Yolo\n",
    "from util import (get_result, input_image, plot_image)\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def to_xyxy(bbox):\n",
    "    if(len(bbox)):\n",
    "        class_num, chances, xcenter, ycenter, width, height = bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3], bbox[0][4], bbox[0][5]\n",
    "        x1, y1 = xcenter-width/2, ycenter-height/2\n",
    "        x2, y2 = xcenter+width/2, ycenter+height/2\n",
    "        return x1, y1, x2, y2\n",
    "    else:\n",
    "        return 0, 0, 0, 0\n",
    "    \n",
    "    \n",
    "def yolo_detection(emotion_model):\n",
    "    #os.chdir(\"C:/Users/jixua/OneDrive/Desktop/Machine learning package/YOLO\")\n",
    "    global img\n",
    "    \n",
    "    model = torch.load('model.pth')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    pTime = 0\n",
    "\n",
    "    if switch == 1:\n",
    "        #cap = cv2.VideoCapture(0)\n",
    "        while True:\n",
    "            if switch == 0:\n",
    "                break\n",
    "            success, img = camera.read()\n",
    "            if img is None:\n",
    "                continue\n",
    "            #imge to tensor \n",
    "            new_img = input_image(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "            new_img = new_img.to(device)\n",
    "            bbox = get_result(model(new_img))\n",
    "            #plot_image(Image.fromarray(img), bbox)\n",
    "            #bbox is in the form of 0, xcenter(normalize), ycenter(normalize), width(normalize), height(normalize)\n",
    "            x1, y1, x2, y2 = to_xyxy(bbox)\n",
    "            img = cv2.rectangle(img,  (int(x1 * img.shape[1]),int(y1* img.shape[0])), (int(x2* img.shape[1]),\n",
    "                                                                                       int(y2* img.shape[0])), (255,0,0), 2)\n",
    "            \n",
    "            bbox_temp = int(x1 * img.shape[1]), int(y1* img.shape[0]), int(x2* img.shape[1]), int(y2* img.shape[0])\n",
    "            crop_img = Image.fromarray(img)\n",
    "            crop_img = crop_img.crop(bbox_temp)\n",
    "            crop_img = crop_img.resize((48, 48))\n",
    "            crop_img_tensor = test_image_transform(crop_img)\n",
    "            crop_img_variable = Variable(crop_img_tensor.unsqueeze(0))\n",
    "            crop_img_variable = crop_img_variable.to(device)\n",
    "            bs, ncrops, c, h, w = crop_img_variable.shape\n",
    "            crop_img_variable = crop_img_variable.view(-1, c, h, w)\n",
    "            \n",
    "            output = emotion_model(crop_img_variable)\n",
    "            output = output.view(bs, ncrops, -1)\n",
    "            output = torch.sum(output, dim=1) / ncrops\n",
    "            \n",
    "            h_x = torch.nn.functional.softmax(output, dim = 1).data.squeeze()\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "            cv2.putText(img, '{}. probability:{:.3f}'.format(labels_class[idx[0].item()], probs[0]), (int(x2* img.shape[1]-10),int(y2* img.shape[0])), cv2.FONT_HERSHEY_PLAIN,1, (255, 0, 255), 2)\n",
    "            cTime = time.time()\n",
    "            fps = 1 / (cTime - pTime)\n",
    "            pTime = cTime\n",
    "            # put fps\n",
    "            #cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            (flag, encodedImage) = cv2.imencode(\".jpg\", img)\n",
    "            if not flag:\n",
    "                continue\n",
    "                \n",
    "            yield(b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + \n",
    "                bytearray(encodedImage) + b'\\r\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/jixua/OneDrive/Desktop/Machine learning package/Web\")\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/video')\n",
    "def detector():\n",
    "    #return Response(real_time_detection(emotion_model), mimetype = 'multipart/x-mixed-replace; boundary=frame')\n",
    "    return Response(yolo_detection(emotion_model), mimetype = 'multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/requests', methods = ['POST', 'GET'])\n",
    "def capturing():\n",
    "    global switch,camera\n",
    "    if request.method == 'POST':\n",
    "        if request.form.get('stop') == 'STOP/START':\n",
    "            if(switch == 1):\n",
    "                switch = 0\n",
    "                camera.release()\n",
    "                cv2.destroyAllWindows()\n",
    "            else:\n",
    "                camera = cv2.VideoCapture(0)\n",
    "                switch = 1\n",
    "    elif request.method == 'GET':\n",
    "        return render_template('index.html')\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(threaded=True)\n",
    "    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
